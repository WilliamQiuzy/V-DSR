{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># VDPM + GPT-5 mini 在 VLM4D 错题上的测试\n\n这个 notebook 测试在加入 VDPM 点云轨迹图后，GPT-5 mini 能否回答正确之前答错的题目。\n\n**在 Colab 上运行:**\n1. Runtime → Change runtime type → GPU (T4)\n2. 按顺序运行所有 cell\n\n**流程:**\n1. 安装 VDPM 依赖 (会自动重启一次)\n2. 下载 10 个测试视频\n3. 用 VDPM 生成点云\n4. 渲染轨迹图\n5. 对比测试 GPT-5 mini"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 1. 安装 VDPM 依赖 (首次运行会重启)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nSETUP_FLAG = '/content/.vdpm_gpt_ready_v7'\n\ndef check_installation():\n    \"\"\"验证所有依赖是否已安装且版本兼容\"\"\"\n    try:\n        import vggt\n        import omegaconf\n        import plotly\n        import openai\n        import torch\n        import kaleido\n        # 检查 plotly 版本是否 >= 6.0\n        major_version = int(plotly.__version__.split('.')[0])\n        if major_version < 6:\n            print(f\"Plotly 版本过低: {plotly.__version__}, 需要 >= 6.0\")\n            return False\n        # 检查 Chrome 是否安装 (可能在多个位置)\n        chrome_paths = [\n            '/usr/bin/google-chrome',\n            '/usr/local/lib/python3.12/dist-packages/choreographer/cli/browser_exe/chrome-linux64/chrome',\n            '/usr/local/lib/python3.11/dist-packages/choreographer/cli/browser_exe/chrome-linux64/chrome',\n            '/usr/local/lib/python3.10/dist-packages/choreographer/cli/browser_exe/chrome-linux64/chrome',\n        ]\n        chrome_found = any(os.path.exists(p) for p in chrome_paths)\n        if not chrome_found:\n            print(\"Chrome 未安装\")\n            return False\n        return True\n    except ImportError:\n        return False\n\nif os.path.exists(SETUP_FLAG) and check_installation():\n    print(\"✓ 已完成安装，继续运行下面的 cell\")\nelse:\n    # 清除旧标记\n    for f in ['/content/.vdpm_gpt_ready_v3', '/content/.vdpm_gpt_ready_v4', '/content/.vdpm_gpt_ready_v5', '/content/.vdpm_gpt_ready_v6', '/content/.vdpm_gpt_ready_v7']:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    print(\"安装 VDPM 依赖...\")\n    \n    # Clone VDPM\n    print(\"\\n[1/6] Clone VDPM...\")\n    !rm -rf /content/vdpm\n    !git clone --depth 1 https://github.com/eldar/vdpm.git /content/vdpm\n    \n    # Fix NumPy\n    print(\"\\n[2/6] Fix NumPy...\")\n    !pip uninstall -y numpy\n    !pip install numpy==1.26.4\n    \n    # Install VGGT\n    print(\"\\n[3/6] Install VGGT...\")\n    !pip install git+https://github.com/facebookresearch/vggt.git@44b3afb\n    \n    # Install other deps\n    print(\"\\n[4/6] Install other deps...\")\n    !pip install roma omegaconf einops jaxtyping\n    \n    print(\"\\n[5/6] Install OpenAI & Plotly...\")\n    !pip install openai aiolimiter tqdm python-dotenv opencv-python pydantic\n    # 强制升级 plotly 到兼容版本\n    !pip install --upgrade \"plotly>=6.1.1\" \"kaleido>=1.2.0\"\n    \n    # 安装 Chrome (Kaleido 需要)\n    print(\"\\n[6/6] Install Chrome for Kaleido...\")\n    # 先安装 Chrome 所需的系统依赖\n    !apt-get update && apt-get install -y libnss3 libatk-bridge2.0-0 libcups2 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libxkbcommon0 libpango-1.0-0 libcairo2 libasound2\n    !plotly_get_chrome\n    \n    # 验证安装\n    print(\"\\n验证安装...\")\n    try:\n        import vggt\n        import omegaconf\n        import plotly\n        import kaleido\n        print(f\"✓ plotly={plotly.__version__}\")\n        print(\"✓ kaleido OK\")\n        print(\"✓ Chrome installed\")\n        \n        # 只有验证通过才标记完成\n        !touch {SETUP_FLAG}\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"✓ 安装完成！正在重启...\")\n        print(\"重启后请从头重新运行所有 cell\")\n        print(\"=\"*50)\n        os._exit(0)\n    except ImportError as e:\n        print(f\"✗ 验证失败: {e}\")\n        print(\"请重新运行此 cell\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 2. 导入和配置"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nos.chdir('/content/vdpm')\nsys.path.insert(0, '/content/vdpm')\n\nimport json\nimport asyncio\nimport base64\nimport hashlib\nimport cv2\nimport requests\nimport random\nimport numpy as np\nimport torch\nimport tempfile\nfrom pathlib import Path\nfrom string import Template\nfrom tqdm import tqdm\nfrom openai import AsyncOpenAI\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# OpenAI API Key\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    # 在 Colab 中手动设置\n    from google.colab import userdata\n    try:\n        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n    except:\n        pass\n\nif not OPENAI_API_KEY:\n    OPENAI_API_KEY = input(\"请输入 OPENAI_API_KEY: \")\n    \nprint(f\"API Key: {OPENAI_API_KEY[:10]}...\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 3. 加载 10 道错题"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 道 GPT-5 mini 之前回答错误的关于运动方向的题目\nSELECTED_QUESTIONS = [\n    {\n        \"id\": \"validation_5\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/baseball.mp4\",\n        \"question\": \"What direction did the ball come from?\",\n        \"choices\": {\"A\": \"right\", \"B\": \"left\", \"C\": \"below\", \"D\": \"above\"},\n        \"answer\": \"left\",\n        \"model_wrong_answer\": \"A (right)\"\n    },\n    {\n        \"id\": \"validation_11\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/basketball-game.mp4\",\n        \"question\": \"How many times did the person with the ball dribble the ball with his left hand?\",\n        \"choices\": {\"A\": 4, \"B\": 8, \"C\": 1, \"D\": 0},\n        \"answer\": 0,\n        \"model_wrong_answer\": \"A (4)\"\n    },\n    {\n        \"id\": \"validation_18\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/bear.mp4\",\n        \"question\": \"Is the bear spinning clockwise or counter-clockwise?\",\n        \"choices\": {\"A\": \"clockwise\", \"B\": \"counter-clockwise\", \"C\": \"there are no bears in the video\", \"D\": \"not spinning\"},\n        \"answer\": \"not spinning\",\n        \"model_wrong_answer\": \"A (clockwise)\"\n    },\n    {\n        \"id\": \"validation_25\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/bike-packing.mp4\",\n        \"question\": \"Which direction is the bike moving towards?\",\n        \"choices\": {\"A\": \"right\", \"B\": \"staying in place\", \"C\": \"away from the camera\", \"D\": \"left\"},\n        \"answer\": \"staying in place\",\n        \"model_wrong_answer\": \"A (right)\"\n    },\n    {\n        \"id\": \"validation_34\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/blackswan.mp4\",\n        \"question\": \"Is the swan spinning clockwise or counter-clockwise?\",\n        \"choices\": {\"A\": \"counter-clockwise\", \"B\": \"both ways\", \"C\": \"clockwise\", \"D\": \"not spinning\"},\n        \"answer\": \"not spinning\",\n        \"model_wrong_answer\": \"C (clockwise)\"\n    },\n    {\n        \"id\": \"validation_37\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/bmx-bumps.mp4\",\n        \"question\": \"From the camera perspective, what direction is the boy moving towards?\",\n        \"choices\": {\"A\": \"left\", \"B\": \"not moving\", \"C\": \"right\", \"D\": \"towards the camera\"},\n        \"answer\": \"left\",\n        \"model_wrong_answer\": \"C (right)\"\n    },\n    {\n        \"id\": \"validation_46\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/bmx-rider.mp4\",\n        \"question\": \"Is the bike rider spinning clockwise or counter-clockwise in the air?\",\n        \"choices\": {\"A\": \"clockwise\", \"B\": \"not spinning\", \"C\": \"counter-clockwise\", \"D\": \"there are no people in the video\"},\n        \"answer\": \"counter-clockwise\",\n        \"model_wrong_answer\": \"A (clockwise)\"\n    },\n    {\n        \"id\": \"validation_54\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/boat.mp4\",\n        \"question\": \"Is the boat rotating clockwise or counter-clockwise?\",\n        \"choices\": {\"A\": \"clockwise\", \"B\": \"not rotating\", \"C\": \"counter-clockwise\", \"D\": \"there are no boats in the video\"},\n        \"answer\": \"not rotating\",\n        \"model_wrong_answer\": \"C (counter-clockwise)\"\n    },\n    {\n        \"id\": \"validation_66\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/breakdance.mp4\",\n        \"question\": \"How many full revolutions does the dancer spin clockwise?\",\n        \"choices\": {\"A\": 5, \"B\": 7, \"C\": 2, \"D\": 0},\n        \"answer\": 2,\n        \"model_wrong_answer\": \"B (7)\"\n    },\n    {\n        \"id\": \"validation_72\",\n        \"video\": \"https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/breakdance-flare.mp4\",\n        \"question\": \"How many full revolutions does the dancer spin counter-clockwise?\",\n        \"choices\": {\"A\": 1, \"B\": 2, \"C\": 4, \"D\": 0},\n        \"answer\": 0,\n        \"model_wrong_answer\": \"B (2)\"\n    }\n]\n\nprint(f\"加载了 {len(SELECTED_QUESTIONS)} 道错题\")\nfor i, q in enumerate(SELECTED_QUESTIONS, 1):\n    video_name = q['video'].split('/')[-1]\n    print(f\"{i}. [{video_name}] {q['question'][:50]}...\")\n    print(f\"   正确答案: {q['answer']}, 之前错误回答: {q['model_wrong_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 4. 下载视频"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "VIDEO_DIR = Path(\"/content/vlm4d_videos\")\nVIDEO_DIR.mkdir(parents=True, exist_ok=True)\n\nfor q in SELECTED_QUESTIONS:\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    video_path = VIDEO_DIR / f\"{video_name}.mp4\"\n    \n    if video_path.exists():\n        print(f\"✓ 已存在: {video_name}.mp4\")\n    else:\n        print(f\"下载: {video_name}.mp4 ...\", end=\" \")\n        resp = requests.get(q['video'])\n        video_path.write_bytes(resp.content)\n        print(\"完成\")\n    \n    q['local_video_path'] = str(video_path)\n\nprint(f\"\\n✓ 所有视频已下载到 {VIDEO_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 5. 用 VDPM 生成点云"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "from omegaconf import OmegaConf\nfrom dpm.model import VDPM\nfrom vggt.utils.load_fn import load_and_preprocess_images\n\n# 加载 VDPM 模型\nprint(\"加载 VDPM 模型...\")\ncfg = OmegaConf.create({\n    'model': {'name': 'dpm-video', 'pretrained': None, 'decoder_depth': 4}\n})\nmodel = VDPM(cfg).to(device)\n\nurl = \"https://huggingface.co/edgarsucar/vdpm/resolve/main/model.pt\"\nweights = torch.hub.load_state_dict_from_url(url, file_name=\"vdpm_model.pt\")\nmodel.load_state_dict(weights, strict=True)\nmodel.eval()\nprint(\"✓ 模型加载完成\")"
  },
  {
   "cell_type": "code",
   "id": "8u7eyerki1j",
   "source": "def extract_frames(video_path, output_dir, sample_hz=1.0):\n    \"\"\"提取视频帧\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n    interval = max(int(fps / sample_hz), 1)\n    \n    paths = []\n    count = 0\n    frame_idx = 0\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % interval == 0:\n            path = output_dir / f\"{frame_idx:04d}.png\"\n            cv2.imwrite(str(path), frame)\n            paths.append(str(path))\n            frame_idx += 1\n        count += 1\n    cap.release()\n    return sorted(paths)\n\n\ndef run_vdpm(video_path, output_dir, ref_frame=0):\n    \"\"\"运行 VDPM 生成点云\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    with tempfile.TemporaryDirectory() as tmp:\n        frame_paths = extract_frames(video_path, tmp)\n        print(f\"  提取了 {len(frame_paths)} 帧\")\n        images = load_and_preprocess_images(frame_paths).to(device)\n    \n    with torch.no_grad():\n        result = model.inference(None, images=images.unsqueeze(0))\n    \n    pointmaps = result['pointmaps']\n    pts_list = [pm['pts3d'].detach().cpu().numpy() for pm in pointmaps]\n    conf_list = [pm['conf'].detach().cpu().numpy() for pm in pointmaps]\n    \n    world_points = np.concatenate(pts_list, axis=0)\n    world_conf = np.concatenate(conf_list, axis=0)\n    \n    num_frames = world_points.shape[0]\n    all_pts, all_conf = [], []\n    \n    for t in range(num_frames):\n        pts = world_points[t, ref_frame, :, :, :].reshape(-1, 3)\n        conf = world_conf[t, ref_frame, :, :].reshape(-1)\n        all_pts.append(pts)\n        all_conf.append(conf)\n    \n    np.savez(output_dir / \"sequence.npz\", points=np.stack(all_pts), conf=np.stack(all_conf))\n    return output_dir\n\n\n# 生成所有视频的点云\nPOINTCLOUD_DIR = Path(\"/content/vlm4d_pointclouds\")\nPOINTCLOUD_DIR.mkdir(parents=True, exist_ok=True)\n\nfor q in SELECTED_QUESTIONS:\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    output_dir = POINTCLOUD_DIR / video_name\n    npz_path = output_dir / \"sequence.npz\"\n    \n    if npz_path.exists():\n        print(f\"✓ {video_name}: 点云已存在\")\n        q['npz_path'] = str(npz_path)\n        continue\n    \n    print(f\"处理: {video_name}\")\n    try:\n        run_vdpm(q['local_video_path'], output_dir)\n        q['npz_path'] = str(npz_path)\n        print(f\"  ✓ 完成\")\n    except Exception as e:\n        print(f\"  ✗ 失败: {e}\")\n    \n    torch.cuda.empty_cache()\n\nprint(\"\\n✓ 点云生成完成\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 6. 渲染轨迹图\n\n颜色说明：**浅青色→深红色** 表示时间从早到晚，即运动方向。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nVDPM 轨迹渲染代码 (内嵌自 render_trajectory.py)\n\n基于 VDPM gradio_demo.py 的 Plotly 渲染逻辑，\n直接从 .npz 文件生成带轨迹的点云图片。\n\"\"\"\n\nimport matplotlib\nimport matplotlib.colors\nimport plotly.graph_objects as go\nfrom typing import Tuple, Optional\n\n# 参数（来自 gradio_demo.py）\nMAX_POINTS_PER_FRAME = 50_000\nTRAIL_LENGTH = 16\nMAX_TRACKS = 200\nSTATIC_THRESHOLD = 0.025\n\n\ndef load_vdpm_data(npz_path: str, video_path: str) -> dict:\n    \"\"\"加载 VDPM 数据和视频帧颜色\"\"\"\n    data = np.load(npz_path)\n    points = data['points']  # (T, N, 3)\n    conf = data['conf']      # (T, N)\n\n    T, N, _ = points.shape\n\n    # 推断分辨率\n    H, W = None, None\n    for h in range(200, 600):\n        if N % h == 0:\n            w = N // h\n            if 0.9 < w / h < 2.1:\n                H, W = h, w\n                break\n\n    if H is None:\n        raise ValueError(f\"无法推断分辨率，点数: {N}\")\n\n    print(f\"点云分辨率: {W}x{H}\")\n\n    # 读取视频帧获取颜色\n    video = cv2.VideoCapture(video_path)\n    images = []\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            break\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0\n        frame_resized = cv2.resize(frame_rgb, (W, H))\n        images.append(frame_resized)\n    video.release()\n\n    # 均匀采样到 T 帧\n    indices = np.linspace(0, len(images) - 1, T, dtype=int)\n    images = np.array([images[i] for i in indices])\n\n    world_points = points.reshape(T, H, W, 3)\n\n    return {\n        'world_points': world_points,\n        'conf': conf.reshape(T, H, W) if conf is not None else None,\n        'images': images,\n    }\n\n\ndef compute_scene_bounds(world_points: np.ndarray):\n    \"\"\"计算场景边界\"\"\"\n    all_pts = world_points.reshape(-1, 3)\n    raw_min = all_pts.min(axis=0)\n    raw_max = all_pts.max(axis=0)\n\n    center = 0.5 * (raw_min + raw_max)\n    half_extent = 0.5 * (raw_max - raw_min) * 1.05\n\n    if np.all(half_extent < 1e-6):\n        half_extent[:] = 1.0\n    else:\n        half_extent[half_extent < 1e-6] = half_extent.max()\n\n    global_min = center - half_extent\n    global_max = center + half_extent\n\n    max_half = half_extent.max()\n    aspectratio = {\n        \"x\": float(half_extent[0] / max_half),\n        \"y\": float(half_extent[1] / max_half),\n        \"z\": float(half_extent[2] / max_half),\n    }\n    return global_min, global_max, aspectratio\n\n\ndef prepare_tracks(\n    world_points: np.ndarray,\n    images: np.ndarray,\n    conf: Optional[np.ndarray],\n    conf_thres: float = 1.5,\n    color_mode: str = \"rainbow\",\n) -> Tuple[Optional[np.ndarray], Optional[list], Optional[np.ndarray]]:\n    \"\"\"准备轨迹数据\"\"\"\n    S, H, W, _ = world_points.shape\n    N = H * W\n    if S < 2 or N == 0:\n        return None, None, None\n\n    tracks_xyz = world_points.reshape(S, N, 3)\n\n    disp = np.linalg.norm(tracks_xyz - tracks_xyz[0:1], axis=-1)\n    dynamic_mask = disp.max(axis=0) > STATIC_THRESHOLD\n\n    if conf is not None:\n        conf_flat = conf.reshape(S, N)\n        conf_score = conf_flat.mean(axis=0)\n        dynamic_mask &= (conf_score >= conf_thres)\n\n    idx_tracks = np.nonzero(dynamic_mask)[0]\n    if idx_tracks.size == 0:\n        return None, None, None\n\n    if idx_tracks.size > MAX_TRACKS:\n        step = int(np.ceil(idx_tracks.size / MAX_TRACKS))\n        idx_tracks = idx_tracks[::step][:MAX_TRACKS]\n\n    tracks_xyz = tracks_xyz[:, idx_tracks, :]\n\n    order = np.argsort(tracks_xyz[0, :, 1])\n    tracks_xyz = tracks_xyz[:, order, :]\n\n    num_tracks = tracks_xyz.shape[1]\n    num_frames = tracks_xyz.shape[0]\n\n    if color_mode == \"depth\":\n        colorscale = []\n        for t in range(num_frames):\n            ratio = t / max(num_frames - 1, 1)\n            r = int(150 + (180 - 150) * ratio)\n            g = int(230 - 230 * ratio)\n            b = int(255 - 225 * ratio)\n            pos = ratio\n            colorscale.append([pos, f\"rgb({r},{g},{b})\"])\n        track_ids = np.arange(num_frames, dtype=float)\n    else:\n        cmap = matplotlib.cm.get_cmap(\"hsv\")\n        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(num_tracks - 1, 1))\n\n        colorscale = []\n        for t in range(num_tracks):\n            r, g, b, _ = cmap(norm(t))\n            r, g, b = int(r * 255), int(g * 255), int(b * 255)\n            pos = t / max(num_tracks - 1, 1)\n            colorscale.append([pos, f\"rgb({r},{g},{b})\"])\n        track_ids = np.arange(num_tracks, dtype=float)\n\n    return tracks_xyz, colorscale, track_ids, color_mode, num_frames\n\n\ndef track_segments_for_frame(\n    tracks_xyz: Optional[np.ndarray],\n    track_ids: Optional[np.ndarray],\n    f: int,\n    trail_length: int = TRAIL_LENGTH,\n    color_mode: str = \"rainbow\",\n    num_frames: int = 1,\n):\n    \"\"\"获取某帧的轨迹线段\"\"\"\n    if tracks_xyz is None or track_ids is None or f <= 0:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n\n    start_t = max(0, f - trail_length)\n    num_tracks = tracks_xyz.shape[1]\n\n    xs, ys, zs, cs = [], [], [], []\n    for j in range(num_tracks):\n        seg = tracks_xyz[start_t: f + 1, j, :]\n        if seg.shape[0] < 2:\n            continue\n\n        xs.extend([seg[:, 0], np.array([np.nan])])\n        ys.extend([seg[:, 1], np.array([np.nan])])\n        zs.extend([seg[:, 2], np.array([np.nan])])\n\n        if color_mode == \"depth\":\n            time_indices = np.arange(start_t, f + 1, dtype=float)\n            cs.append(np.concatenate([time_indices, np.array([np.nan])]))\n        else:\n            cs.append(np.full(seg.shape[0] + 1, track_ids[j], dtype=float))\n\n    x = np.concatenate(xs) if xs else np.array([])\n    y = np.concatenate(ys) if ys else np.array([])\n    z = np.concatenate(zs) if zs else np.array([])\n    c = np.concatenate(cs) if cs else np.array([])\n\n    return x, y, z, c\n\n\ndef sample_frame_points(\n    world_points: np.ndarray,\n    images: np.ndarray,\n    conf: Optional[np.ndarray],\n    frame_idx: int,\n    conf_thres: float = 1.5,\n    max_points: int = MAX_POINTS_PER_FRAME,\n):\n    \"\"\"采样某帧的点和颜色\"\"\"\n    S, H, W, _ = world_points.shape\n    pts = world_points[frame_idx].reshape(-1, 3)\n    cols = (images[frame_idx].reshape(-1, 3) * 255).astype(np.uint8)\n\n    mask = np.ones(pts.shape[0], dtype=bool)\n    if conf is not None:\n        conf_flat = conf[frame_idx].reshape(-1)\n        mask &= (conf_flat >= conf_thres)\n\n    pts = pts[mask]\n    cols = cols[mask]\n\n    n = pts.shape[0]\n    if n > max_points:\n        step = int(np.ceil(n / max_points))\n        pts = pts[::step]\n        cols = cols[::step]\n\n    colors_str = [f\"#{r:02x}{g:02x}{b:02x}\" for r, g, b in cols]\n    return pts, colors_str\n\n\ndef render_frame_with_tracks(\n    data: dict,\n    frame_idx: int,\n    output_path: str,\n    conf_thres: float = 1.5,\n    width: int = 1200,\n    height: int = 900,\n    show_tracks: bool = True,\n    camera_eye: tuple = (0.0, 0.0, -2.0),\n    camera_center: tuple = (0.0, 0.0, 0.5),\n    color_mode: str = \"rainbow\",\n) -> str:\n    \"\"\"渲染某帧的点云和轨迹，保存为图片\"\"\"\n    world_points = data['world_points']\n    images = data['images']\n    conf = data.get('conf')\n\n    S = world_points.shape[0]\n    frame_idx = min(frame_idx, S - 1)\n\n    global_min, global_max, aspectratio = compute_scene_bounds(world_points)\n\n    if show_tracks:\n        result = prepare_tracks(\n            world_points, images, conf, conf_thres, color_mode\n        )\n        tracks_xyz, colorscale, track_ids, actual_color_mode, num_frames = result\n        if actual_color_mode == \"depth\":\n            track_cmax = max(num_frames - 1, 1)\n        else:\n            track_cmax = max(len(track_ids) - 1, 1) if track_ids is not None else 1\n    else:\n        tracks_xyz, colorscale, track_ids = None, None, None\n        track_cmax = 1\n        actual_color_mode = color_mode\n        num_frames = S\n\n    pts, cols = sample_frame_points(\n        world_points, images, conf, frame_idx, conf_thres\n    )\n\n    x, y, z, c = track_segments_for_frame(\n        tracks_xyz, track_ids, frame_idx,\n        color_mode=actual_color_mode, num_frames=num_frames\n    )\n\n    traces = [\n        go.Scatter3d(\n            x=pts[:, 0],\n            y=pts[:, 1],\n            z=pts[:, 2],\n            mode=\"markers\",\n            marker=dict(size=2, color=cols),\n            showlegend=False,\n            name=\"points\",\n        ),\n    ]\n\n    if show_tracks and len(x) > 0:\n        traces.append(\n            go.Scatter3d(\n                x=x,\n                y=y,\n                z=z,\n                mode=\"lines\",\n                line=dict(\n                    width=3,\n                    color=c if c is not None and c.size else None,\n                    colorscale=colorscale if colorscale else None,\n                    cmin=0,\n                    cmax=track_cmax,\n                ),\n                hoverinfo=\"skip\",\n                showlegend=False,\n                name=\"tracks\",\n            )\n        )\n\n    fig = go.Figure(data=traces)\n\n    scene_cfg = dict(\n        xaxis=dict(visible=False, showbackground=False, range=[float(global_min[0]), float(global_max[0])]),\n        yaxis=dict(visible=False, showbackground=False, range=[float(global_min[1]), float(global_max[1])]),\n        zaxis=dict(visible=False, showbackground=False, range=[float(global_min[2]), float(global_max[2])]),\n        aspectmode=\"manual\",\n        aspectratio=aspectratio,\n        camera=dict(\n            eye=dict(x=0.0, y=0.0, z=-1.0),\n            center=dict(x=0.0, y=0.0, z=0.0),\n            up=dict(x=0.0, y=-1.0, z=0.0),\n        ),\n        bgcolor='white',\n    )\n\n    fig.update_layout(\n        margin=dict(l=0, r=0, t=0, b=0),\n        scene=scene_cfg,\n        showlegend=False,\n        width=width,\n        height=height,\n        paper_bgcolor='white',\n    )\n\n    fig.write_image(output_path, scale=2)\n    print(f\"已保存: {output_path}\")\n    return output_path\n\n\ndef render_trajectory_image(\n    npz_path: str,\n    video_path: str,\n    output_path: str,\n    frame_idx: int = -1,\n    conf_thres: float = 1.5,\n    width: int = 1200,\n    height: int = 900,\n    camera_eye: tuple = (0.0, 0.0, -2.0),\n    camera_center: tuple = (0.0, 0.0, 0.5),\n    color_mode: str = \"rainbow\",\n) -> str:\n    \"\"\"主函数：从 npz 和视频生成轨迹图片\"\"\"\n    print(f\"加载数据: {npz_path}\")\n    data = load_vdpm_data(npz_path, video_path)\n\n    T = data['world_points'].shape[0]\n    if frame_idx == -1:\n        frame_idx = T - 1\n\n    print(f\"渲染帧 {frame_idx}/{T-1}, 颜色模式: {color_mode}\")\n    return render_frame_with_tracks(\n        data, frame_idx, output_path,\n        conf_thres=conf_thres,\n        width=width,\n        height=height,\n        camera_eye=camera_eye,\n        camera_center=camera_center,\n        color_mode=color_mode,\n    )\n\n\nprint(\"轨迹渲染代码已加载\")"
  },
  {
   "cell_type": "code",
   "id": "51o2iya0wj4",
   "source": "# 渲染所有视频的轨迹图\nRENDER_DIR = \"/content/vdpm/vdpm_renders\"  # 使用绝对路径\nos.makedirs(RENDER_DIR, exist_ok=True)\n\nfor q in tqdm(SELECTED_QUESTIONS, desc=\"渲染轨迹图\"):\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    npz_path = q.get('npz_path')\n    \n    if not npz_path or not os.path.exists(npz_path):\n        print(f\"跳过 {video_name}: 无点云\")\n        continue\n    \n    render_path = os.path.join(RENDER_DIR, f\"{video_name}.png\")\n    \n    if os.path.exists(render_path):\n        print(f\"已存在: {video_name}.png\")\n        q['render_path'] = render_path  # 即使已存在也要设置路径！\n        continue\n    \n    try:\n        render_trajectory_image(\n            npz_path=npz_path,\n            video_path=q['local_video_path'],\n            output_path=render_path,\n            frame_idx=-1,  # 最后一帧\n            color_mode=\"depth\",  # 由浅到深\n        )\n        q['render_path'] = render_path\n        print(f\"✓ 渲染完成: {video_name}.png\")\n    except Exception as e:\n        print(f\"✗ 渲染失败 {video_name}: {e}\")\n\n# 验证所有轨迹图\nprint(\"\\n轨迹图状态:\")\nfor q in SELECTED_QUESTIONS:\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    has_render = 'render_path' in q and os.path.exists(q.get('render_path', ''))\n    status = \"✓\" if has_render else \"✗\"\n    print(f\"  {status} {video_name}: {q.get('render_path', '无')}\")\n\nprint(\"\\n轨迹图渲染完成\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 7. 准备 Prompt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# 常量\nMAX_TOKENS = 2048  # 增加到 2048 避免输出被截断 (之前 1024 导致 Q3/Q7 被截断)\nGENERATION_TEMPERATURE = 1.0\nTOTAL_FRAMES = 32\n\n# 原始 Prompt (仅视频帧)\nORIGINAL_PROMPT = Template(\"\"\"\nQuestion: $question\n$optionized_str\n\nAnswer the given multiple-choice question step by step. Begin by explaining your reasoning process clearly. In the last sentence of your response, you must conclude by stating the final answer using the following format: 'Therefore, the final answer is: $$LETTER' (without quotes), where $$LETTER must be only one of the options (A or B or C or D). Think step by step before answering.\"\"\")\n\n# 新 Prompt (视频帧 + 轨迹图)\nVDPM_PROMPT = Template(\"\"\"\nYou are given video frames and a 3D point cloud trajectory visualization.\n\n**About the trajectory image:**\n- The trajectory image shows the motion paths of objects in 3D space\n- Line colors indicate time: **light cyan = early position, dark red = later position**\n- This helps you understand which direction objects are moving towards\n- If trajectories are short or clustered, the object may not be moving much\n- If trajectories are long and directional, the object is clearly moving in that direction\n\n**Instructions:**\n1. First, analyze the video frames to understand the scene\n2. Then, use the trajectory image to determine the actual motion direction\n3. The color gradient (light→dark) shows you exactly where objects moved from and to\n\nQuestion: $question\n$optionized_str\n\nAnswer the given multiple-choice question step by step. Use both the video frames AND the trajectory visualization to reason about motion. In the last sentence of your response, you must conclude by stating the final answer using the following format: 'Therefore, the final answer is: $$LETTER' (without quotes), where $$LETTER must be only one of the options (A or B or C or D). Think step by step before answering.\"\"\")\n\nprint(\"Prompt 模板已准备\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 8. 工具函数"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_frames(video_path, total_frames):\n    \"\"\"从视频中均匀采样帧并转为 base64\"\"\"\n    video = cv2.VideoCapture(video_path)\n    if not video.isOpened():\n        raise ValueError(f\"Could not open video file: {video_path}\")\n    \n    try:\n        base64_frames = []\n        while True:\n            success, frame = video.read()\n            if not success:\n                break\n            _, buffer = cv2.imencode('.jpg', frame)\n            frame_base64 = base64.b64encode(buffer).decode('utf-8')\n            base64_frames.append(frame_base64)\n        \n        # 均匀采样\n        random.seed(42)\n        if total_frames == 1:\n            selected_indices = [np.random.choice(range(len(base64_frames)))]\n        else:\n            selected_indices = np.linspace(0, len(base64_frames) - 1, total_frames, dtype=int)\n        \n        selected_base64_frames = [base64_frames[i] for i in selected_indices]\n        return selected_base64_frames\n    finally:\n        video.release()\n\n\ndef read_image_as_base64(image_path):\n    \"\"\"读取图片并转为 base64\"\"\"\n    with open(image_path, \"rb\") as f:\n        return base64.b64encode(f.read()).decode('utf-8')\n\n\ndef prepare_message_original(query, total_frames, prompt_template):\n    \"\"\"准备原始消息 (仅视频帧)\"\"\"\n    # 准备文本\n    optionized_list = [f\"{key}: {value}\" for key, value in query['choices'].items()]\n    optionized_str = \"\\n\".join(optionized_list)\n    qa_text = prompt_template.substitute(question=query['question'], optionized_str=optionized_str)\n    \n    # 准备视频帧\n    base64_frames = read_video_frames(query['local_video_path'], total_frames)\n    \n    content = [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame}\"}}\n        for frame in base64_frames\n    ]\n    content.append({\"type\": \"text\", \"text\": qa_text})\n    \n    return [{\"role\": \"user\", \"content\": content}]\n\n\ndef prepare_message_with_vdpm(query, total_frames, prompt_template):\n    \"\"\"准备带 VDPM 轨迹图的消息\"\"\"\n    # 准备文本\n    optionized_list = [f\"{key}: {value}\" for key, value in query['choices'].items()]\n    optionized_str = \"\\n\".join(optionized_list)\n    qa_text = prompt_template.substitute(question=query['question'], optionized_str=optionized_str)\n    \n    # 准备视频帧\n    base64_frames = read_video_frames(query['local_video_path'], total_frames)\n    \n    content = [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame}\"}}\n        for frame in base64_frames\n    ]\n    \n    # 添加轨迹图 (放在视频帧之后)\n    if 'render_path' in query and os.path.exists(query['render_path']):\n        trajectory_base64 = read_image_as_base64(query['render_path'])\n        content.append({\n            \"type\": \"image_url\",\n            \"image_url\": {\"url\": f\"data:image/png;base64,{trajectory_base64}\"}\n        })\n    \n    content.append({\"type\": \"text\", \"text\": qa_text})\n    \n    return [{\"role\": \"user\", \"content\": content}]\n\n\nprint(\"工具函数已准备\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 9. API 调用函数"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel\nimport aiolimiter\nfrom tqdm.asyncio import tqdm_asyncio\n\n# LLM 评估输出格式 (与 VLM4D 官方一致)\nclass EvaluationOutput(BaseModel):\n    extracted_answer: str\n    correct: bool\n\n# 评估 Prompt (来自 VLM4D utils/eval_utils.py)\nEVAL_INSTRUCTION = \"\"\"Your task is to evaluate whether the model's final answer is correct by comparing it to the ground-truth answer provided for the given question.\n\nYou should first extract the final answer from the model's response, and then compare the extracted answer with the choice that matches the ground-truth answer to determine its correctness.\nOutput your response in the following structured format:\n{\n    \"extracted_answer\": // str value \"A\" \"B\" \"C\" \"D\", followed by a colon and the corresponding answer text, e.g., \"A: Answer A text\". If the model's response does not contain a valid choice and reasoning, then \"No Valid Answer\".\n    \"correct\": // boolean value, True if the extracted answer matches the ground-truth answer (correct choice), False otherwise (\"No Valid Answer\" is also considered False).\n}\n\"\"\"\n\n\ndef prepare_evaluation_message(example, response):\n    \"\"\"准备评估消息 (来自 VLM4D utils/eval_utils.py)\"\"\"\n    optionized_list = [f\"{key}: {value}\" for key, value in example['choices'].items()]\n    optionized_str = \"\\n\".join(optionized_list)\n    question_context = f\"Question: {example['question']}\\n\\nOptions:\\n{optionized_str}\"\n    gt_answer = f\"Ground Truth Answer: {example['answer']}\"\n    model_response = f\"Model Response to the Question: {response}\"\n    \n    user_prompt = f\"{question_context}\\n\\n{gt_answer}\\n\\n{model_response}\"\n    \n    return [\n        {\"role\": \"system\", \"content\": EVAL_INSTRUCTION},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n\n\nasync def _throttled_openai_chat_completion_acreate(\n    client,\n    model,\n    messages,\n    temperature,\n    max_tokens,\n    top_p,\n    limiter,\n    question_id=\"unknown\",  # 添加问题ID用于调试\n):\n    \"\"\"单次 API 调用（带重试）\"\"\"\n    async with limiter:\n        for attempt in range(10):\n            try:\n                response = await client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=temperature,\n                    max_completion_tokens=max_tokens,\n                    top_p=top_p,\n                )\n                # 检查是否有 refusal 或 finish_reason 问题\n                choice = response.choices[0]\n                if choice.finish_reason == \"content_filter\":\n                    print(f\"⚠️ {question_id}: 内容被过滤 (content_filter)\")\n                elif choice.finish_reason == \"length\":\n                    print(f\"⚠️ {question_id}: 输出被截断 (length)\")\n                elif hasattr(choice.message, 'refusal') and choice.message.refusal:\n                    print(f\"⚠️ {question_id}: 模型拒绝回答: {choice.message.refusal}\")\n                \n                return response\n            except Exception as e:\n                error_str = str(e).lower()\n                if \"rate_limit\" in error_str:\n                    print(f\"Rate limit exceeded, retrying (attempt {attempt+1})...\")\n                    await asyncio.sleep(random.randint(10, 20))\n                elif \"bad_request\" in error_str:\n                    print(f\"Bad request for {question_id}: {e}\")\n                    return None\n                elif \"context_length\" in error_str or \"too many tokens\" in error_str:\n                    print(f\"⚠️ {question_id}: 输入太长，超过上下文限制: {e}\")\n                    return None\n                else:\n                    print(f\"Error for {question_id} (attempt {attempt+1}): {e}\")\n                    await asyncio.sleep(random.randint(5, 10))\n        print(f\"⚠️ {question_id}: 10次重试后仍失败\")\n        return None\n\n\nasync def generate_from_openai_chat_completion(\n    client,\n    messages,\n    engine_name,\n    temperature=1.0,\n    max_tokens=512,\n    top_p=1.0,\n    requests_per_minute=150,\n    question_ids=None,  # 添加问题ID列表用于调试\n):\n    \"\"\"批量调用 OpenAI API\"\"\"\n    delay = 60.0 / requests_per_minute\n    limiter = aiolimiter.AsyncLimiter(1, delay)\n    \n    if question_ids is None:\n        question_ids = [f\"Q{i+1}\" for i in range(len(messages))]\n    \n    async_responses = [\n        _throttled_openai_chat_completion_acreate(\n            client,\n            model=engine_name,\n            messages=message,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            limiter=limiter,\n            question_id=qid,\n        )\n        for message, qid in zip(messages, question_ids)\n    ]\n    \n    responses = await tqdm_asyncio.gather(*async_responses, desc=\"API 调用\")\n    \n    outputs = []\n    for i, (response, qid) in enumerate(zip(responses, question_ids)):\n        if response is None:\n            print(f\"⚠️ {qid}: API 返回 None\")\n            outputs.append(\"\")\n        else:\n            try:\n                content = response.choices[0].message.content\n                if content:\n                    outputs.append(content)\n                else:\n                    # 打印完整的 choice 信息用于调试\n                    choice = response.choices[0]\n                    print(f\"⚠️ {qid}: 回答内容为空!\")\n                    print(f\"   finish_reason: {choice.finish_reason}\")\n                    if hasattr(choice.message, 'refusal') and choice.message.refusal:\n                        print(f\"   refusal: {choice.message.refusal}\")\n                    outputs.append(\"\")\n            except Exception as e:\n                print(f\"⚠️ {qid}: 提取回答失败 - {e}\")\n                outputs.append(\"\")\n    \n    return outputs\n\n\nasync def _throttled_eval_call(client, model, messages, limiter):\n    \"\"\"单次评估调用 (o4-mini 需要用 max_completion_tokens)\"\"\"\n    async with limiter:\n        for _ in range(10):\n            try:\n                response = await client.beta.chat.completions.parse(\n                    model=model,\n                    messages=messages,\n                    temperature=1.0,\n                    max_completion_tokens=1024,  # o4-mini 需要用 max_completion_tokens\n                    top_p=1.0,\n                    response_format=EvaluationOutput,\n                )\n                return response.choices[0].message.parsed\n            except Exception as e:\n                if \"rate_limit\" in str(e).lower():\n                    await asyncio.sleep(random.randint(10, 20))\n                else:\n                    print(f\"Eval error: {e}\")\n                    await asyncio.sleep(random.randint(5, 10))\n        return None\n\n\nasync def get_acc_async(examples, client, eval_model=\"o4-mini\"):\n    \"\"\"评估所有响应 (来自 VLM4D utils/eval_utils.py)\"\"\"\n    evaluation_messages = [\n        prepare_evaluation_message(example, example['response'])\n        for example in examples\n    ]\n    \n    # 批量评估\n    delay = 60.0 / 1000  # 1000 requests per minute for eval\n    limiter = aiolimiter.AsyncLimiter(1, delay)\n    \n    tasks = [_throttled_eval_call(client, eval_model, msg, limiter) for msg in evaluation_messages]\n    outputs = await tqdm_asyncio.gather(*tasks, desc=\"评估中\")\n    \n    # 统计结果\n    count = 0\n    results = []\n    for example, output in zip(examples, outputs):\n        result = {\n            \"id\": example[\"id\"],\n            \"question\": example[\"question\"],\n            \"choices\": example[\"choices\"],\n            \"response\": example[\"response\"],\n            \"ground_truth_answer\": example[\"answer\"],\n        }\n        try:\n            result[\"extracted_answer\"] = output.extracted_answer\n            result[\"correct\"] = output.correct\n        except Exception as e:\n            result[\"extracted_answer\"] = \"\"\n            result[\"correct\"] = False\n            print(f\"Error: {e}\")\n        \n        results.append(result)\n        count += result[\"correct\"]\n    \n    return count / len(examples) if examples else 0, results\n\n\nprint(\"API 函数和评估器已加载 (与 VLM4D 官方一致)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 10. 运行 VDPM 测试"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-5-mini\"\nclient = AsyncOpenAI(api_key=OPENAI_API_KEY)\n\nprint(\"开始测试...\")\nprint(f\"模型: {MODEL_NAME}\")\nprint(f\"帧数: {TOTAL_FRAMES}\")\nprint(f\"题目数: {len(SELECTED_QUESTIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: 获取模型回答\nprint(\"=\"*50)\nprint(\"Step 1: 获取 GPT-5 mini 回答 (视频帧 + VDPM 轨迹图)\")\nprint(\"=\"*50)\n\n# 自动补充缺失的路径（处理重新运行的情况）\nRENDER_DIR = \"/content/vdpm/vdpm_renders\"\nVIDEO_DIR = Path(\"/content/vlm4d_videos\")\nPOINTCLOUD_DIR = Path(\"/content/vlm4d_pointclouds\")\n\nfor q in SELECTED_QUESTIONS:\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    \n    # 补充 local_video_path\n    if 'local_video_path' not in q:\n        q['local_video_path'] = str(VIDEO_DIR / f\"{video_name}.mp4\")\n    \n    # 补充 npz_path\n    if 'npz_path' not in q:\n        npz_path = POINTCLOUD_DIR / video_name / \"sequence.npz\"\n        if npz_path.exists():\n            q['npz_path'] = str(npz_path)\n    \n    # 补充 render_path\n    if 'render_path' not in q:\n        render_path = os.path.join(RENDER_DIR, f\"{video_name}.png\")\n        if os.path.exists(render_path):\n            q['render_path'] = render_path\n\n# 准备所有消息\nall_messages = []\nvalid_indices = []  # 记录有轨迹图的题目索引\nquestion_ids = []  # 记录问题ID用于调试\n\nfor i, q in enumerate(SELECTED_QUESTIONS):\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    \n    # 检查是否有轨迹图\n    if 'render_path' not in q or not os.path.exists(q.get('render_path', '')):\n        print(f\"跳过 {q['id']}: 无轨迹图 (render_path={q.get('render_path', '无')})\")\n        continue\n    \n    message = prepare_message_with_vdpm(q, TOTAL_FRAMES, VDPM_PROMPT)\n    all_messages.append(message)\n    valid_indices.append(i)\n    question_ids.append(f\"{q['id']} ({video_name})\")\n    \n    # 添加 question_type 字段 (评估需要)\n    q['question_type'] = 'multiple-choice'\n    \n    # 打印图片大小信息\n    render_size = os.path.getsize(q['render_path']) / 1024  # KB\n    print(f\"  {q['id']}: 轨迹图大小 {render_size:.1f} KB\")\n\nprint(f\"\\n准备了 {len(all_messages)} 个有效问题\")\n\nif len(all_messages) == 0:\n    print(\"\\n⚠️  没有有效问题！请检查:\")\n    print(\"1. 是否运行了点云生成 cell\")\n    print(\"2. 是否运行了轨迹图渲染 cell\")\n    print(\"3. 轨迹图文件是否存在于\", RENDER_DIR)\nelse:\n    # 批量调用 API\n    base_rate = 100\n    requests_per_minute = int(base_rate / (TOTAL_FRAMES ** 0.5))\n    print(f\"请求速率: {requests_per_minute}/min\")\n\n    responses = await generate_from_openai_chat_completion(\n        client=client,\n        messages=all_messages,\n        engine_name=MODEL_NAME,\n        temperature=GENERATION_TEMPERATURE,\n        max_tokens=MAX_TOKENS,\n        top_p=1.0,\n        requests_per_minute=requests_per_minute,\n        question_ids=question_ids,  # 传递问题ID用于调试\n    )\n\n    # 将响应添加到对应的问题中，并打印哪些失败了\n    print(\"\\n回答状态:\")\n    for idx, response, qid in zip(valid_indices, responses, question_ids):\n        SELECTED_QUESTIONS[idx]['response'] = response\n        SELECTED_QUESTIONS[idx]['vdpm_response'] = response\n        \n        if response:\n            print(f\"  ✓ {qid}: 收到回答 ({len(response)} 字符)\")\n        else:\n            print(f\"  ✗ {qid}: 回答为空!\")\n\n    success_count = sum(1 for r in responses if r)\n    print(f\"\\n获得 {success_count}/{len(responses)} 个有效响应\")"
  },
  {
   "cell_type": "code",
   "id": "9mo1k217onj",
   "source": "# 查看模型完整回答\nprint(\"=\"*50)\nprint(\"模型完整回答\")\nprint(\"=\"*50)\n\nfor i, q in enumerate(SELECTED_QUESTIONS, 1):\n    video_name = q['video'].split('/')[-1].replace('.mp4', '')\n    response = q.get('response', '')\n    \n    print(f\"\\n--- [{q['id']}] {video_name} ---\")\n    print(f\"问题: {q['question']}\")\n    print(f\"选项: {q['choices']}\")\n    print(f\"正确答案: {q['answer']}\")\n    print(f\"之前错误回答: {q['model_wrong_answer']}\")\n    print(f\"\\n模型回答:\")\n    if response:\n        # 显示完整回答，如果太长则截断\n        if len(response) > 800:\n            print(f\"{response[:800]}...\")\n        else:\n            print(response)\n    else:\n        print(\"(无回答 - 跳过)\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dl2hfu2j7q6",
   "source": "# Step 2: 使用 LLM 评估\nprint(\"=\"*50)\nprint(\"Step 2: 使用 o4-mini 进行 LLM 评估\")\nprint(\"=\"*50)\n\n# 只评估有回答的问题\nexamples_to_eval = [q for q in SELECTED_QUESTIONS if q.get('response')]\n\nprint(f\"评估 {len(examples_to_eval)} 个回答...\")\n\nvdpm_accuracy, eval_results = await get_acc_async(examples_to_eval, client, eval_model=\"o4-mini\")\n\nprint(f\"\\n\" + \"=\" * 50)\nprint(f\"VDPM 准确率: {vdpm_accuracy:.0%} ({int(vdpm_accuracy * len(eval_results))}/{len(eval_results)})\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 11. 结果汇总"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# 详细评估结果\nprint(\"=\"*60)\nprint(\"详细评估结果 (LLM 评估)\")\nprint(\"=\"*60)\n\n# 构建 ID 到评估结果的映射\neval_map = {r['id']: r for r in eval_results}\n\ncorrect_ids = []\nwrong_ids = []\nskipped_ids = []\n\nfor q in SELECTED_QUESTIONS:\n    qid = q['id']\n    if qid in eval_map:\n        r = eval_map[qid]\n        if r['correct']:\n            correct_ids.append(qid)\n        else:\n            wrong_ids.append(qid)\n    else:\n        skipped_ids.append(qid)\n\nprint(f\"\\n模型: {MODEL_NAME}\")\nprint(f\"评估模型: o4-mini\")\nprint(f\"帧数: {TOTAL_FRAMES}\")\nprint(f\"测试题目: {len(SELECTED_QUESTIONS)}\")\nprint(f\"\\nVDPM 准确率: {vdpm_accuracy:.0%} ({len(correct_ids)}/{len(eval_results)})\")\nprint(f\"(这些题目之前 GPT-5 mini 全部答错)\")\n\nprint(f\"\\n✓ 正确题目 ({len(correct_ids)}): {', '.join(correct_ids) if correct_ids else '无'}\")\nprint(f\"✗ 错误题目 ({len(wrong_ids)}): {', '.join(wrong_ids) if wrong_ids else '无'}\")\nif skipped_ids:\n    print(f\"⊘ 跳过题目 ({len(skipped_ids)}): {', '.join(skipped_ids)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"逐题详情\")\nprint(\"=\"*60)\n\nfor i, r in enumerate(eval_results, 1):\n    status = \"✓\" if r['correct'] else \"✗\"\n    print(f\"\\n{status} [{r['id']}]\")\n    print(f\"   问题: {r['question'][:60]}...\")\n    print(f\"   正确答案: {r['ground_truth_answer']}\")\n    print(f\"   LLM提取答案: {r['extracted_answer']}\")\n    \n    # 找到原始问题以获取之前的错误回答\n    orig_q = next((q for q in SELECTED_QUESTIONS if q['id'] == r['id']), None)\n    if orig_q:\n        print(f\"   之前错误回答: {orig_q['model_wrong_answer']}\")\n    \n    if r['correct']:\n        print(f\"   ★ VDPM 帮助纠正了这道题!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 12. 保存结果"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# 保存结果\nresults = {\n    \"model\": MODEL_NAME,\n    \"eval_model\": \"o4-mini\",\n    \"total_frames\": TOTAL_FRAMES,\n    \"vdpm_accuracy\": vdpm_accuracy,\n    \"num_correct\": len(correct_ids),\n    \"num_total\": len(eval_results),\n    \"correct_ids\": correct_ids,\n    \"wrong_ids\": wrong_ids,\n    \"skipped_ids\": skipped_ids,\n    \"eval_results\": eval_results,  # 完整评估结果\n}\n\noutput_file = \"/content/vdpm_test_results.json\"\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n\nprint(f\"结果已保存到: {output_file}\")\n\n# 下载结果\nfrom google.colab import files\nfiles.download(output_file)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}